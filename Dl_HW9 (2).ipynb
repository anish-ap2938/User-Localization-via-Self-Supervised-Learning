{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkAqPRzbAvDe"
      },
      "outputs": [],
      "source": [
        "# My Kaggle ID: Anish Panicker(ap2938)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import h5py\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_dir = '/kaggle/input/w9-itr1/'\n",
        "labelled_dir = os.path.join(base_dir, 'labelled', 'labelled_data')\n",
        "unlabelled_dir = os.path.join(base_dir, 'unlabelled_data', 'unlabelled_data')\n",
        "test_dir = os.path.join(base_dir, 'test', 'test')\n",
        "def load_data_from_directory_dynamic(directory_path, labelled=True, max_samples=None):\n",
        "    H_Re_list = []\n",
        "    H_Im_list = []\n",
        "    SNR_list = []\n",
        "    Pos_list = [] if labelled else None\n",
        "    total_loaded = 0\n",
        "\n",
        "    file_paths_h5 = glob.glob(os.path.join(directory_path, '*.h5')) + glob.glob(os.path.join(directory_path, '*.hdf5'))\n",
        "\n",
        "    if file_paths_h5:\n",
        "        print(\"Loading data from HDF5 files...\")\n",
        "        for file_path in file_paths_h5:\n",
        "            with h5py.File(file_path, 'r') as f:\n",
        "                num_samples = f['H_Re'].shape[0]\n",
        "                if max_samples:\n",
        "                    remaining = max_samples - total_loaded\n",
        "                    if remaining <= 0:\n",
        "                        break\n",
        "                    load_count = min(remaining, num_samples)\n",
        "                else:\n",
        "                    load_count = num_samples\n",
        "\n",
        "                # Slice the datasets\n",
        "                H_Re = f['H_Re'][:load_count]\n",
        "                H_Im = f['H_Im'][:load_count]\n",
        "                SNR = f['SNR'][:load_count]\n",
        "                H_Re_list.append(H_Re)\n",
        "                H_Im_list.append(H_Im)\n",
        "                SNR_list.append(SNR)\n",
        "                if labelled and 'Pos' in f.keys():\n",
        "                    Pos = f['Pos'][:load_count]\n",
        "                    Pos_list.append(Pos)\n",
        "\n",
        "                total_loaded += load_count\n",
        "                print(f\"Loaded {load_count} samples from {file_path}. Total loaded: {total_loaded}\")\n",
        "\n",
        "                if max_samples and total_loaded >= max_samples:\n",
        "                    break\n",
        "    else:\n",
        "        raise ValueError(f\"No supported data files found in directory '{directory_path}'.\")\n",
        "\n",
        "    if not H_Re_list:\n",
        "        raise ValueError(f\"No valid data found in directory '{directory_path}'.\")\n",
        "\n",
        "    H_Re = np.concatenate(H_Re_list, axis=0)\n",
        "    H_Im = np.concatenate(H_Im_list, axis=0)\n",
        "    SNR = np.concatenate(SNR_list, axis=0)\n",
        "    if labelled:\n",
        "        if not Pos_list:\n",
        "            raise ValueError(\"Labelled data is missing 'Pos' key in some files.\")\n",
        "        Pos = np.concatenate(Pos_list, axis=0)\n",
        "        return H_Re, H_Im, SNR, Pos\n",
        "    else:\n",
        "        return H_Re, H_Im, SNR\n",
        "print(\"Loading labelled data...\")\n",
        "H_Re_labelled, H_Im_labelled, SNR_labelled, Pos_labelled = load_data_from_directory_dynamic(labelled_dir, labelled=True)\n",
        "print(f\"Labelled data: H_Re={H_Re_labelled.shape}, H_Im={H_Im_labelled.shape}, SNR={SNR_labelled.shape}, Pos={Pos_labelled.shape}\")\n",
        "unlabelled_sample_size = 4000\n",
        "print(f\"\\nLoading unlabelled data with up to {unlabelled_sample_size} samples...\")\n",
        "H_Re_unlabelled, H_Im_unlabelled, SNR_unlabelled = load_data_from_directory_dynamic(unlabelled_dir, labelled=False, max_samples=unlabelled_sample_size)\n",
        "print(f\"Unlabelled data: H_Re={H_Re_unlabelled.shape}, H_Im={H_Im_unlabelled.shape}, SNR={SNR_unlabelled.shape}\")\n",
        "print(\"\\nLoading test data...\")\n",
        "H_Re_test, H_Im_test, SNR_test = load_data_from_directory_dynamic(test_dir, labelled=False)\n",
        "print(f\"Test data: H_Re={H_Re_test.shape}, H_Im={H_Im_test.shape}, SNR={SNR_test.shape}\")\n",
        "def preprocess_data_simplified(H_Re, H_Im, SNR):\n",
        "    H_Re_mean = np.mean(H_Re, axis=-1)\n",
        "    H_Im_mean = np.mean(H_Im, axis=-1)\n",
        "    SNR_mean = np.mean(SNR, axis=-1)\n",
        "    H_complex = H_Re_mean + 1j * H_Im_mean\n",
        "    H_mag = np.abs(H_complex)\n",
        "    SNR_norm = (SNR_mean - np.mean(SNR_mean)) / np.std(SNR_mean)\n",
        "    SNR_expanded = np.expand_dims(SNR_norm, axis=-1)\n",
        "    SNR_tiled = np.tile(SNR_expanded, (1, 1, H_mag.shape[2]))\n",
        "    H_final = np.expand_dims(H_mag, axis=-1)\n",
        "    H_final = np.concatenate([H_final, SNR_tiled[..., np.newaxis]], axis=-1)\n",
        "    H_final[H_final == 0] = 1e-6\n",
        "    H_final = (H_final - np.mean(H_final)) / np.std(H_final)\n",
        "\n",
        "    return H_final\n",
        "\n",
        "print(\"Preprocessing data with simplified method...\")\n",
        "H_labelled = preprocess_data_simplified(H_Re_labelled, H_Im_labelled, SNR_labelled)\n",
        "H_unlabelled = preprocess_data_simplified(H_Re_unlabelled, H_Im_unlabelled, SNR_unlabelled)\n",
        "H_test = preprocess_data_simplified(H_Re_test, H_Im_test, SNR_test)\n",
        "print(f\"Preprocessed Labelled Data Shape: {H_labelled.shape}\")\n",
        "print(f\"Preprocessed Unlabelled Data Shape: {H_unlabelled.shape}\")\n",
        "print(f\"Preprocessed Test Data Shape: {H_test.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NOrUrbG6En5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_optimized_autoencoder(input_shape):\n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "    x = layers.Conv2D(8, (3,3), activation='relu', padding='same')(input_layer)\n",
        "    x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
        "    x = layers.Conv2D(16, (3,3), activation='relu', padding='same')(x)\n",
        "    encoded = layers.MaxPooling2D((2,2), padding='same')(x)\n",
        "    x = layers.Conv2D(16, (3,3), activation='relu', padding='same')(encoded)\n",
        "    x = layers.UpSampling2D((2,2))(x)\n",
        "    x = layers.Conv2D(8, (3,3), activation='relu', padding='same')(x)\n",
        "    x = layers.UpSampling2D((2,2))(x)\n",
        "    decoded = layers.Conv2D(input_shape[-1], (3,3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    autoencoder = models.Model(input_layer, decoded)\n",
        "    encoder = models.Model(input_layer, encoded)\n",
        "    return autoencoder, encoder\n",
        "\n",
        "input_shape = H_unlabelled.shape[1:]\n",
        "autoencoder, encoder = build_optimized_autoencoder(input_shape)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "H_ua_train, H_ua_val = train_test_split(H_unlabelled, test_size=0.1, random_state=42)\n",
        "print(\"\\nTraining the autoencoder...\")\n",
        "autoencoder.fit(\n",
        "    H_ua_train, H_ua_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    validation_data=(H_ua_val, H_ua_val),\n",
        "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]\n",
        ")\n",
        "print(\"\\nExtracting encoded features...\")\n",
        "encoded_labelled = encoder.predict(H_labelled, batch_size=32)\n",
        "encoded_test = encoder.predict(H_test, batch_size=32)\n",
        "encoded_labelled_flat = encoded_labelled.reshape(encoded_labelled.shape[0], -1)\n",
        "encoded_test_flat = encoded_test.reshape(encoded_test.shape[0], -1)\n",
        "X = encoded_labelled_flat\n",
        "y = Pos_labelled\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kBw-bwxKBryr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_optimized_regressor(input_dim):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Dense(64, activation='relu', input_dim=input_dim))\n",
        "    model.add(layers.Dense(32, activation='relu'))\n",
        "    model.add(layers.Dense(16, activation='relu'))\n",
        "    model.add(layers.Dense(3))\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "regressor = build_optimized_regressor(X_train.shape[1])\n",
        "regressor.summary()\n",
        "print(\"\\nTraining the regressor...\")\n",
        "regressor.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=20,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "id": "1KAzQsxYBr1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nPredicting on test data...\")\n",
        "y_test_pred = regressor.predict(encoded_test_flat)\n",
        "submission = pd.DataFrame({\n",
        "    'id': np.arange(len(y_test_pred)),\n",
        "    'x': y_test_pred[:, 0],\n",
        "    'y': y_test_pred[:, 1],\n",
        "    'z': y_test_pred[:, 2]\n",
        "})\n",
        "submission.to_csv('submission1.csv', index=False)\n",
        "print(\"Submission file 'submission1.csv' created successfully.\")\n"
      ],
      "metadata": {
        "id": "yxq67QYUBr4f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}